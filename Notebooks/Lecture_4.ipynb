{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4: Fine-tuning a Language Model using Huggingface\n",
    "Building is the best way   of learning AI/ML.\n",
    "\n",
    "## Fine-Tuning Our Language Model\n",
    "Language modeling predicts words in a sentence. There are different types. In causal language modeling, the task is to predict the next toklen in a sequence of tokens using only the tokens that came before it. \n",
    "\n",
    "### Huggingface\n",
    "Community and data science center for building, training and deploying ML models based on open source software. \n",
    "\n",
    "### Loading up a dataset\n",
    "Use the Datasets Library, with three main feature:\n",
    "* Efficient way to load and process data from raw files (CSV/JSON/text) or in-memory data \n",
    "* A simple way to access asnd share datasets with the research and practitioner community\n",
    "* Interoperable with DL frameworks like pandas, NumPy, PyTorch and TensorFlow\n",
    "\n",
    "**Def**. SQuAD dataset (Stanfrod Question Answering Dataset) consists of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. \n",
    "\n",
    "Fine-tune on SQuAD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/Users/maxcasas/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"squad\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove columns that we are not going to use, and use the map function to add a special token that GPT2 uses to mark the end of a document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [00:06<00:00, 14561.11ex/s]\n",
      "100%|██████████| 10570/10570 [00:00<00:00, 17161.07ex/s]\n"
     ]
    }
   ],
   "source": [
    "def add_end_of_text(example: dict) -> dict:\n",
    "    example[\"question\"] = example[\"question\"] + \"<|endoftext|>\"\n",
    "    return example\n",
    "\n",
    "dataset = dataset.remove_columns([\"id\", \"title\", \"context\", \"answers\"])\n",
    "dataset = dataset.map(add_end_of_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?<|endoftext|>',\n",
       " 'What is in front of the Notre Dame Main Building?<|endoftext|>',\n",
       " 'The Basilica of the Sacred heart at Notre Dame is beside to which structure?<|endoftext|>',\n",
       " 'What is the Grotto at Notre Dame?<|endoftext|>',\n",
       " 'What sits on top of the Main Building at Notre Dame?<|endoftext|>',\n",
       " 'When did the Scholastic Magazine of Notre dame begin publishing?<|endoftext|>',\n",
       " \"How often is Notre Dame's the Juggler published?<|endoftext|>\",\n",
       " 'What is the daily student paper at Notre Dame called?<|endoftext|>',\n",
       " 'How many student news papers are found at Notre Dame?<|endoftext|>',\n",
       " 'In what year did the student paper Common Sense begin publication at Notre Dame?<|endoftext|>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the structure fo some entries\n",
    "dataset[\"train\"][\"question\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "Process the data in an acceptable format for the model. Use a tokenizer, which prepares the inputs for a model. \n",
    "\n",
    "A tokenization pipeline in HF comprises several steps:\n",
    "1. Normalization (any cleanup of the text that is deemed necessary, such as removing spaces or accents, Unicode normalization, etc.)\n",
    "2. Pre-tokenization (splitting the input into words)\n",
    "3. Running the input through the model (using the pre-tokenized words to produce a sequence of tokens)\n",
    "4. Post-processing (adding the special tokens of the tokenizer, generating the attention mask and token type IDs)\n",
    "\n",
    "For example: Hello how are U tday?\n",
    "1. hello how are u tday?\n",
    "2. [hello, how, are, u, tday,]\n",
    "3. [hello, how, are, u, ##ay, ?]\n",
    "4. [CLS, hwllo, how, are, u, td, ##ay, ?, SEP]\n",
    "\n",
    "For tokenization, there are three main subword tokenization algorithms: BPE, WordPiece and Unigram.\n",
    "\n",
    "Since tokenization processes are model-specific, if we want to fine-tune the model on new data, we need to instantiate the tokenizer using the name of the model, to make sure we use the same rules that were used when the model was pretrained. This is done by the AutoTokenizer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 762/762 [00:00<00:00, 70.5kB/s]\n",
      "Downloading: 100%|██████████| 0.99M/0.99M [00:00<00:00, 2.17MB/s]\n",
      "Downloading: 100%|██████████| 446k/446k [00:00<00:00, 990kB/s] \n",
      "Downloading: 100%|██████████| 1.29M/1.29M [00:00<00:00, 2.34MB/s]\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"distilgpt2\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert a sample sentence to tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'Ġtoken', 'izer', 'Ġis', 'Ġbeing', 'Ġapplied', 'Ġin', 'ĠCS', '197', 'Ġat', 'Har', 'vard', '.', '<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "sequence = (\"This tokenizer is being applied in CS197 at\"\n",
    "            \"Harvard.<|endoftext|>\")\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these models, the space before a word is part of a word, so they are converted in a special character Ġ in the tokenizer. To convert tokens into numbers, the tokenizer has a vocabulary, which is the part we download when we instantiate it wit the pretrained method. We need to use the same vocabulary used when the model was pretrained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1212, 11241, 7509, 318, 852, 5625, 287, 9429, 24991, 379, 13587, 10187, 13, 50256]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1212, 11241, 7509, 318, 852, 5625, 287, 9429, 24991, 379, 13587, 10187, 13, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = (\"This tokenizer is being applied in CS197 at\"\n",
    "            \"Harvard.<|endoftext|>\")\n",
    "tokenizer(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dictionary with 2 important items:\n",
    "1. input_ids: the indices corresponding to each token in the sentence\n",
    "2. attention_mask: indicates whether a token should be attended to or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#0:   0%|          | 0/22 [00:00<?, ?ba/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "#0:   9%|▉         | 2/22 [00:01<00:10,  1.85ba/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  14%|█▎        | 3/22 [00:01<00:06,  2.87ba/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  18%|█▊        | 4/22 [00:01<00:05,  3.58ba/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  23%|██▎       | 5/22 [00:01<00:04,  4.16ba/s]\n",
      "\n",
      "#0:  27%|██▋       | 6/22 [00:01<00:04,  3.91ba/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  32%|███▏      | 7/22 [00:02<00:04,  3.58ba/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  36%|███▋      | 8/22 [00:02<00:03,  3.89ba/s]\n",
      "#0:  41%|████      | 9/22 [00:06<00:20,  1.56s/ba]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "#0:  45%|████▌     | 10/22 [00:07<00:14,  1.17s/ba]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  50%|█████     | 11/22 [00:07<00:09,  1.14ba/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  55%|█████▍    | 12/22 [00:07<00:07,  1.36ba/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  59%|█████▉    | 13/22 [00:08<00:05,  1.64ba/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  64%|██████▎   | 14/22 [00:08<00:03,  2.06ba/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  68%|██████▊   | 15/22 [00:08<00:02,  2.44ba/s]\n",
      "#0:  73%|███████▎  | 16/22 [00:08<00:02,  2.78ba/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "#0:  77%|███████▋  | 17/22 [00:08<00:01,  3.09ba/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  82%|████████▏ | 18/22 [00:09<00:01,  3.44ba/s]\n",
      "#0:  86%|████████▋ | 19/22 [00:09<00:00,  3.76ba/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "#0:  91%|█████████ | 20/22 [00:09<00:00,  3.63ba/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  95%|█████████▌| 21/22 [00:09<00:00,  3.99ba/s]\n",
      "#2: 100%|██████████| 22/22 [00:09<00:00,  2.21ba/s]\n",
      "#1: 100%|██████████| 22/22 [00:10<00:00,  2.20ba/s]\n",
      "#0: 100%|██████████| 22/22 [00:10<00:00,  2.18ba/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "#3: 100%|██████████| 22/22 [00:10<00:00,  2.12ba/s]\n",
      "#0:   0%|          | 0/3 [00:00<?, ?ba/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  33%|███▎      | 1/3 [00:00<00:00,  5.12ba/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  67%|██████▋   | 2/3 [00:00<00:00,  5.86ba/s]\n",
      "\n",
      "#0: 100%|██████████| 3/3 [00:00<00:00,  6.70ba/s]\n",
      "\n",
      "\n",
      "#3: 100%|██████████| 3/3 [00:00<00:00,  7.64ba/s]\n",
      "\n",
      "#1: 100%|██████████| 3/3 [00:00<00:00,  5.96ba/s]\n",
      "\n",
      "#2: 100%|██████████| 3/3 [00:00<00:00,  5.58ba/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"question\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=['question']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where we used the Datasets map function. By setting batched = True, we process multiple elements of the dataset at once and increase the number of processes with num_proc=4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "For CLM, one of the data preparation steps is to concatenate the different examples together, and then split them into chunks of equal size. This is so that we can have a common length across all examples without needing to pad. We use chunks defined by block_size of 128. The option batched=True lets us change the number of examples in the datasets by returning a different number of examples than we got. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "#0:   0%|          | 0/22 [00:00<?, ?ba/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "#0:   5%|▍         | 1/22 [00:00<00:11,  1.76ba/s]\n",
      "#0:   9%|▉         | 2/22 [00:00<00:06,  3.02ba/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "#0:  14%|█▎        | 3/22 [00:00<00:04,  3.83ba/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  18%|█▊        | 4/22 [00:01<00:05,  3.15ba/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  23%|██▎       | 5/22 [00:01<00:04,  3.80ba/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  27%|██▋       | 6/22 [00:01<00:03,  4.44ba/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  36%|███▋      | 8/22 [00:02<00:03,  4.50ba/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  41%|████      | 9/22 [00:02<00:02,  4.57ba/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "#0:  45%|████▌     | 10/22 [00:02<00:02,  4.18ba/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  50%|█████     | 11/22 [00:02<00:02,  4.15ba/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  59%|█████▉    | 13/22 [00:03<00:01,  4.84ba/s]\n",
      "\n",
      "#0:  64%|██████▎   | 14/22 [00:03<00:01,  5.42ba/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  68%|██████▊   | 15/22 [00:03<00:01,  5.78ba/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  73%|███████▎  | 16/22 [00:03<00:01,  5.99ba/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  77%|███████▋  | 17/22 [00:03<00:01,  4.65ba/s]\n",
      "\u001b[A\n",
      "#0:  82%|████████▏ | 18/22 [00:04<00:00,  4.30ba/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  86%|████████▋ | 19/22 [00:04<00:00,  4.56ba/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  91%|█████████ | 20/22 [00:04<00:00,  4.51ba/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  95%|█████████▌| 21/22 [00:04<00:00,  4.90ba/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0: 100%|██████████| 22/22 [00:04<00:00,  4.46ba/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "#2: 100%|██████████| 22/22 [00:05<00:00,  4.17ba/s]\n",
      "#1: 100%|██████████| 22/22 [00:05<00:00,  4.13ba/s]\n",
      "#0:   0%|          | 0/3 [00:00<?, ?ba/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  33%|███▎      | 1/3 [00:00<00:00,  3.74ba/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  67%|██████▋   | 2/3 [00:00<00:00,  3.43ba/s]\n",
      "\n",
      "#3: 100%|██████████| 3/3 [00:00<00:00,  5.42ba/s]\n",
      "\n",
      "#2: 100%|██████████| 3/3 [00:00<00:00,  4.80ba/s]\n",
      "#0: 100%|██████████| 3/3 [00:00<00:00,  4.46ba/s]\n",
      "#1: 100%|██████████| 3/3 [00:00<00:00,  4.34ba/s]\n"
     ]
    }
   ],
   "source": [
    "block_size = 128\n",
    "def group_texts(examples):\n",
    "    # Repeat concatenation for input_ids and other keys\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "\n",
    "    # Populate each of the input_ids and other keys\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)] for k, t in concatenated_examples.items()\n",
    "    }\n",
    "\n",
    "    # Add labels because we'll need it as the output\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2514, 4150, 750, 262, 5283, 5335, 7910, 1656, 287, 1248, 3365, 287, 406, 454, 8906, 4881, 30, 50256, 2061, 318, 287, 2166, 286, 262, 23382, 20377, 8774, 11819, 30, 50256, 464, 32520, 3970, 286, 262, 17380, 2612, 379, 23382, 20377, 318, 13970, 284, 543, 4645, 30, 50256, 2061, 318, 262, 10299, 33955, 379, 23382, 20377, 30, 50256, 2061, 10718, 319, 1353, 286, 262, 8774, 11819, 379, 23382, 20377, 30, 50256, 2215, 750, 262, 3059, 349, 3477, 11175, 286, 23382, 288, 480, 2221, 12407, 30, 50256, 2437, 1690, 318, 23382, 20377, 338, 262, 39296, 1754, 3199, 30, 50256, 2061, 318, 262, 4445, 3710, 3348, 379, 23382, 20377, 1444, 30, 50256, 2437, 867, 3710, 1705, 9473, 389, 1043, 379, 23382, 20377, 30, 50256, 818, 644, 614, 750, 262, 3710, 3348]\n"
     ]
    }
   ],
   "source": [
    "print(lm_datasets['train']['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode function to go from our encoded ids back to the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?<|endoftext|>What is in front of the Notre Dame Main Building?<|endoftext|>The Basilica of the Sacred heart at Notre Dame is beside to which structure?<|endoftext|>What is the Grotto at Notre Dame?<|endoftext|>What sits on top of the Main Building at Notre Dame?<|endoftext|>When did the Scholastic Magazine of Notre dame begin publishing?<|endoftext|>How often is Notre Dame's the Juggler published?<|endoftext|>What is the daily student paper at Notre Dame called?<|endoftext|>How many student news papers are found at Notre Dame?<|endoftext|>In what year did the student paper\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets['train']['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a smaller version of the data so we can fine-tune our model in a reasonable amount of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = \\\n",
    "    lm_datasets['train'].shuffle(seed=42).select(range(100))\n",
    "small_eval_dataset = \\\n",
    "    lm_datasets['validation'].shuffle(seed=42).select(range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal language modeling\n",
    "Define training arguments and set p our Trainer. The Trainer class provides an API for feature-complete training in PyTorch for most standard use cases. \n",
    "\n",
    "We will push this model to the Hub, as HF platform where anyone can share and explore models, datassets and demos. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 336M/336M [01:02<00:00, 5.62MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/Users/maxcasas/.pyenv/versions/tests_aladdin_max/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 100\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 39\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaxcasas\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/maxcasas/Documents/Portfolio/Courses/CS_197_Harvard_AI_Research_Experiences/Notebooks/wandb/run-20221015_185110-1qe1lgyy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/maxcasas/huggingface/runs/1qe1lgyy\" target=\"_blank\">distilgpt2-squad</a></strong> to <a href=\"https://wandb.ai/maxcasas/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 13/39 [19:54<40:36, 93.72s/it] ***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n",
      "                                               \n",
      " 33%|███▎      | 13/39 [22:28<40:36, 93.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.7186625003814697, 'eval_runtime': 153.0631, 'eval_samples_per_second': 0.653, 'eval_steps_per_second': 0.085, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 26/39 [47:33<29:38, 136.81s/it]  ***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    f\"{model_checkpoint}-squad\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = small_train_dataset,\n",
    "    eval_dataset = small_eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model. Because we want our model to assign high probabilities to sentences that are real, we seek a model that assigns the highgest probability to the test set. The metric we use is perplexity, the inverse probability of the test set normalized by the number of words in the test set. A lower perplexity is better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload our final model and tokenizer to the hub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"gpt2-squad\")\n",
    "model.push_to_hub(\"gpt2-squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation with our fine-tuned model\n",
    "To autocomplete some questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxcasas/.pyenv/versions/tests_aladdin_max/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: 100%|██████████| 1.00k/1.00k [00:00<00:00, 333kB/s]\n",
      "Downloading: 100%|██████████| 318M/318M [01:04<00:00, 5.21MB/s] \n",
      "Downloading: 100%|██████████| 261/261 [00:00<00:00, 114kB/s]\n",
      "Downloading: 100%|██████████| 779k/779k [00:00<00:00, 1.64MB/s] \n",
      "Downloading: 100%|██████████| 446k/446k [00:00<00:00, 964kB/s] \n",
      "Downloading: 100%|██████████| 2.01M/2.01M [00:00<00:00, 2.77MB/s]\n",
      "Downloading: 100%|██████████| 99.0/99.0 [00:00<00:00, 52.1kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"rajpurkar/gpt2-squad\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rajpurkar/gpt2-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize some text, including some context and the start of a question\n",
    "start_text = (\"A speedrun is a playthrough of a video game, \\\n",
    "or section of a video game, with the goal of \\\n",
    "completing it as fast as possible. Speedruns \\\n",
    "often follow planned routes, which may incorporate sequence \\\n",
    "breaking, and might exploit glitches that allow sections to \\\n",
    "be skipped or completed more quickly than intended. \")\n",
    "\n",
    "prompt = \"What is the\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    start_text + prompt,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\"\n",
    ")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A speedrun is a playthrough of a video game, or section of a video game, with the goal of completing it as fast as possible. Speedruns often follow planned routes, which may incorporate sequence breaking, and might exploit glitches that allow sections to be skipped or completed more quickly than intended. What is the name of the speedrun in an early video game?<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Pass the input into the model for generation\n",
    "prompt_length = len(tokenizer.decode(inputs[0]))\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.9,\n",
    "    num_return_sequences=1,\n",
    ")\n",
    "\n",
    "generated = prompt + tokenizer.decode(outputs[0])[prompt_length + 1:]\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('tests_aladdin_max')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8c11a393b6c36709b056596a1138ad17cf1a37eeed0bd51d6d4e3803b5c9090"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
