{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5: Fine-Tuning a Vision Tranformer using Lightning\n",
    "Image classification: given an image, which of the following classes is it an image of.\n",
    "\n",
    "## Lightning\n",
    "The DL framework with batteries included. It is a layer on top of Pytorch to organize code to remove boilerplate: it abstracts away all the engineering complexity needed for scale.\n",
    "\n",
    "The HuggingFace Trainer API can be seen as a framework similar to PyTorch Lightning in the sense that it also abstracts the training away using a Trainer object. However, contrary to PyTorch Lightning, it is not meant to be a general framework. Rather, it is made especially for fine-tuning Transformer-based models available in the HuggingFace Transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/Users/maxcasas/.pyenv/versions/tests_aladdin_max/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --quiet \"setuptools==59.5.0\" \"pytorch-lightning>=1.4\" \"matplotlib\" \"torch>=1.8\" \"ipython[notebook]\" \"torchmetrics>=0.7\" \"torchvision\" \"seaborn\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "\n",
    "DATASET_PATH = os.environ.get(\"PATH_DATASETS\", \"data/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop(\n",
    "            (32, 32), scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.49139968, 0.48215841, 0.44653091],\n",
    "            [0.24703223, 0.24348513, 0.26158784]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.49139968, 0.48215841, 0.44653091],\n",
    "        [0.24703223, 0.24348513, 0.26158784]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Composing transformations. The compositions are performed in sequence. We horizontally flip the image randomly with a given probability (default 0.5). A crop of the original image is made: the crop has a random area (H*W) and a random aspect ratio. It is resized to the given size. scale (tuple or float) specifies the lower and upper bounds for the random area of the crop before resizing; while ratio (tuple or float) specifies the lower and upper bounds for the random aspect ration of the crop, before resizing. The ToTensor converts a PIL image or numpy ndarray HxWxC in the range [0, 255] to a torch.FloatTensor of shape CxHxw in the range [0.0, 1.0]. The last one normalizes a tensor image with mean and standard deviation. It will normalize each channel of the input using the precomputed means and standard deviation for the CIGAR dataset that we will use. The constants correspond to the values that scale and shift the data to a zero mean and standard deviation of one. \n",
    "\n",
    "The transformations of train are different from test, because the train transforms help augment the data to give the dataset more examples, but in test time, we don't want to corrupt the examples by performing augmentations like cropping them. Tip: Test time augmentations where multiple augmented images are passed through the network and their outputs averaged to get a more performant model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CIFAR10(\n",
    "    root=DATASET_PATH, train=True, download=True, transform=train_transform\n",
    ")\n",
    "val_dataset = CIFAR10(\n",
    "    root=DATASET_PATH, train=True, download=True, transform=test_transform\n",
    ")\n",
    "test_dataset = CIFAR10(\n",
    "    root=DATASET_PATH, train=False, download=True, transform=test_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're loading the same dataset as in the train dataset, but with different transformations. We're applying the same transform to the test set as we do to the val set because we want the validation set to help us pick a model that will perform well on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "pl.seed_everything(42)\n",
    "train_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
    "_, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets pseudo-random number generators and sets a couple of environment variables. The train_dataset and val_dataset loaded the same data and transformed it in two different ways. Here it looks like we’re able to make the train_set and val_set use different sets of images, which is what we’d like to evaluate generalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "# Visualize some examples\n",
    "NUM_IMAGES = 4\n",
    "CIFAR_IMAGES = torch.stack(\n",
    "    [val_set[idx][0] for idx in range(NUM_IMAGES)], dim=0\n",
    ")\n",
    "\n",
    "img_grid = torchvision.utils.make_grid(CIFAR_IMAGES, nrow=4, normalize = True, pad_value = 0.9)\n",
    "img_grid = img_grid.permute(1, 2, 0)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title(\"Image examples of the CIFAR10 dataset\")\n",
    "plt.imshow(img_grid)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(\n",
    "    train_set, batch_size=128,\n",
    "    shuffle = True, drop_last=True, pin_memory=True, num_workers=4)\n",
    "val_loader = data.DataLoader(\n",
    "    val_set, batch_size=128,\n",
    "    shuffle = False, drop_last=False, pin_memory=True, num_workers=4)\n",
    "test_loader = data.DataLoader(\n",
    "    test_dataset, batch_size=128,\n",
    "    shuffle = False, drop_last=False, pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataloader combines a dataset and a sampler, and provides an iterable over the given dataset. It allows us to iterate over a dataset in batches given by the batch size. Shuffles at every epoch if true, which improves performance. This is beause gradient descent relies on randomization to get out of local minimas. drop_last drops the last incomplete batch if the dataset size is not divisible by the batch size if True. num_workers specifies how many subprocess to use for data loading. \n",
    "\n",
    "num_workers?\n",
    "* =0 means ONLY the main process will load batches (that can be a bottleneck)\n",
    "* =1 means only one workers (just not the main process) will load data, but it will still be slow\n",
    "* The performance of high num_workers depends on the batch size and your machine\n",
    "* A general place to start is to set it eqal to the **number of CPU cores in that machine**. Use os.cpu_count(), but depending on your batch size, you may overflow RAM memory\n",
    "* Increasing the number will increase your CPU memory consumption\n",
    "* The best thing is to increase slowly and stop once there is no more im provement in your training speed. For debugging purposes or for dataloaders that load very small datasets, i is desirable to set it equal to 0\n",
    "\n",
    "# Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def img_to_patch(x, patch_size, flatten_channels=True):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        x - Tensor representing the image of shape [B, C, H, W]\n",
    "        patch_size - Number of pixels per dimension of the patches (integer)\n",
    "        flatten_channels - If True, the patches will be returned in a flattened \n",
    "        format as a feature vector instead of an image grid\n",
    "    \"\"\"\n",
    "\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.reshape(\n",
    "        B, \n",
    "        C,\n",
    "        torch.div(H, patch_size, rounding_mode=\"trunc\"),\n",
    "        patch_size,\n",
    "        torch.div(W, patch_size, rounding_mode=\"floor\"),\n",
    "        patch_size\n",
    "    )\n",
    "\n",
    "    x = x.permute(0, 2, 4, 1, 3, 5) # [B, H', W', C, p_H, p_W]\n",
    "    x = x.flatten(1, 2) # [B, H' * W', C, p_H, p_W]\n",
    "\n",
    "    if flatten_channels:\n",
    "        x = x.flatten(2, 4) # [B, H' * W', C * p_H * p_W]\n",
    "    \n",
    "    return x\n",
    "\n",
    "img_patches = img_to_patch(\n",
    "    CIFAR_IMAGES, patch_size=4, flatten_channels=Falsec\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Visio Transformer is a model for image classification that views images as sequences of smaller patches. So as a preprocessing step, we split an image of, for example, 32x32 pixels into a grid of 8x8 of size 4x4 each. The Batch and Channels dimensions are untouched, and we're working to transform the Height and Wifdth into 4 pieces: H', p_H, 2', p_W.\n",
    "\n",
    "The permute operations are getting us to the point at which we will have H'*W' patches for every image, and we can visualize them by looking at (C, p_H, p_W). \n",
    "\n",
    "Then, we combine (flatten) the height and width dimension so that we have one vector of (C*p_H*p_W) elements for each of the H'*W' patches. Each of those patches is considered to be a \"word\"/\"token\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(CIFAR_IMAGES.shape[0], 1, figsize=(14, 3))\n",
    "fig.suptitle(\"Images as input sequences of patches\")\n",
    "for i in range(CIFAR_IMAGES.shape[0]):\n",
    "    img_grid = torchvision.utils.make_grid(\n",
    "        img_patches[i], nrow=64, normalize = True, pad_value = 0.9\n",
    "    )\n",
    "    img_grid = img_grid.permute(1, 2, 0)\n",
    "    ax[i].imshow(img_grid)\n",
    "    ax[i].axis(\"off\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The make_grid function takes in a 4D mini-batch Tensor of shape (BxCxHxW) or a list of images all of the same size; nrow sets the number of images displayed in each row of the grid, normalize shifts the image to the range (0,1) and pad_value sets the value for the padded pixels. \n",
    "\n",
    "# Neural Net Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        inp_x = self.layer_norm_1(x)\n",
    "        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n",
    "        x = x + self.linear(self.layer_norm_2(x))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the forward method we compute output Tensors from input Tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,\n",
    "        hidden_dim,\n",
    "        num_channels, \n",
    "        num_heads, \n",
    "        num_layers,\n",
    "        num_classes,\n",
    "        patch_size, \n",
    "        num_patches,\n",
    "        dropout=0.0\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # Layers/Networks\n",
    "        self.input_layer = nn.Linear(\n",
    "            num_channels * (patch_size**2), embed_dim\n",
    "        )\n",
    "\n",
    "        self.transformer = nn.Sequential(\n",
    "            *[AttentionBlock(embed_dim, hidden_dim, num_heads, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim), nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Parameters/Embeddings\n",
    "        self.cls_token = nn.Parameter(\n",
    "            torch.randn(1, 1, embed_dim)\n",
    "        )\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1, num_patches + 1, embed_dim)\n",
    "        )\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A linear projection layer that maps the input patches (each of num_channels * patch_size**2) to a feature vector of larger size (embed dim)\n",
    "* A multi-layer perceptron MLP that takes an output feature vector and maps it to a classification prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    x = img_to_patch(x, self.patch_size)\n",
    "    B, T, _ = x.shape\n",
    "    x = self.input_layer(x)\n",
    "\n",
    "    cls_token = self.cls_token.repeat(B, 1, 1)\n",
    "    x = torch.cat([cls_token, x], dim=1)\n",
    "    x = x + self.pos_embedding[:, :T + 1]\n",
    "    \n",
    "    x = self.dropout(x)\n",
    "    x = x.transpose(0, 1)\n",
    "    x = self.transformer(x)\n",
    "\n",
    "    cls = x[0]\n",
    "    out = self.mlp_head(cls)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are doing a sum of the positional embeddings with our x. Notice how pos_embeddings is of shape [1, 65, 256] and x is of shape [B, 65, 256] and yet we're able to sum them up, applying the pos_embeddings to every sample in the batch. This is broadcasting. \n",
    "\n",
    "# Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class ViT(pl.LightningModule):\n",
    "    def __init__(self, model_kwargs, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = VisionTransformer(**model_kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.hparams.lr)\n",
    "        lr_scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer, milestones=[100, 150], gamma=0.1\n",
    "        )\n",
    "        \n",
    "        return [optimizer], [lr_scheduler]\n",
    "    \n",
    "    def _calculate_loss(self, batch, mode=\"train\"):\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        acc = (preds.argmax(dim=1) == labels).float().mean()\n",
    "\n",
    "        self.log(\"%s_loss\" % mode, loss, prog_bar=True)\n",
    "        self.log(\"%s_acc\" % mode, acc, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._calculate_loss(batch, mode=\"train\")\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode=\"val\")\n",
    "\n",
    "    def test_step(self, batch,  batch_idx):\n",
    "        self._calculate_loss(batch, mode=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A LightningModule organizes your PyTorch code into sections:\n",
    "* Computations\n",
    "* Forward: Used for inference only (separate from training_step)\n",
    "* Optimizer and scheduler (through configure_optimizers). The optimizer takes in the parameters and determines how the parameters are updated. The scheduler contains the optimizer as a member and alters its parameters learning rates. We don't need to worry about these for now\n",
    "* Training Loop (training_step)\n",
    "* Validation Loop (validation_step)\n",
    "* Test Loop (test_step)\n",
    "\n",
    "All of the loops use _calculate_loss, which computes the cross_entropy loss for the batch comparing the predictions (pred) of the model with the labels, logging the accuracy in the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = os.environ.get(\n",
    "    \"PATH_CHECKPOINT\",\n",
    "    \"saved_models/VisionTransformers/\")\n",
    "\n",
    "def train_model(**kwargs):\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=os.path.join(CHECKPOINT_PATH, \"ViT\"),\n",
    "        fast_dev_run=5,\n",
    "    )\n",
    "\n",
    "    pl.seed_everything(42)  # To be reproducible\n",
    "    model = ViT(**kwargs)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    test_result = trainer.test(\n",
    "        model, dataloaders=test_loader, verbose=False)\n",
    "    return model, test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic use of the trainer is to initialize and then fit the model using the train_loader and the val_loader. We use the test method on the Trainer using the test loader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, results = train_model(\n",
    "    model_kwargs={\n",
    "        \"embed_dim\": 256,\n",
    "        \"hidden_dim\": 512,\n",
    "        \"num_heads\": 8,\n",
    "        \"num_layers\": 6,\n",
    "        \"patch_size\": 4,\n",
    "        \"num_channels\": 3,\n",
    "        \"num_patches\": 64,\n",
    "        \"num_classes\": 10,\n",
    "        \"dropout\": 0.2,\n",
    "    },\n",
    "    lr=3e-4,\n",
    ")\n",
    "print(\"Results\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('tests_aladdin_max')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8c11a393b6c36709b056596a1138ad17cf1a37eeed0bd51d6d4e3803b5c9090"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
